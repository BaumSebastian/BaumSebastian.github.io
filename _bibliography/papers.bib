---
---

@string{aps = {American Physical Society,}}

@article{LINDEMANN2021650,
title = {A survey on long short-term memory networks for time series prediction},
journal = {Procedia CIRP},
volume = {99},
pages = {650-655},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.088},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121003796},
author = {Benjamin Lindemann and Timo MÃ¼ller and Hannes Vietz and Nasser Jazdi and Michael Weyrich},
keywords = {Recurrent Neural Networks, Long short-term memory, Autoencoder, Sequence-to-Sequence Networks, Time Series Prediction},
abstract = {Recurrent neural networks and exceedingly Long short-term memory (LSTM) have been investigated intensively in recent years due to their ability to model and predict nonlinear time-variant system dynamics. The present paper delivers a comprehensive overview of existing LSTM cell derivatives and network architectures for time series prediction. A categorization in LSTM with optimized cell state representations and LSTM with interacting cell states is proposed. The investigated approaches are evaluated against defined requirements being relevant for an accurate time series prediction. These include short-term and long-term memory behavior, the ability for multimodal and multi-step ahead predictions and the according error propagation. Sequence-to-sequence networks with partially conditioning outperform the other approaches, such as bidirectional or associative networks, and are best suited to fulfill the requirements.},
selected={true}
}